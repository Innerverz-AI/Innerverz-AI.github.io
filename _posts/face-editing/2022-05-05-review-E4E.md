---
layout: post
title:  "Designing an Encoder for StyleGAN Image Manipulation (Encoder4Editing)"
author: 이유경
categories: [Encoder4Editing, Face Editing, GAN Inversion, Latent Space, StyleGAN]
image: assets/images/logo-face-editing.jpeg
---

# Getting Ready
## Different Style Spaces
우선 네 가지 intermediate latent space: $W$, $W^k$, $W_\*$, $W_*^k$에 대해 알아보자. 

- StyleGAN의 mapping function은 latent code $z \in Z = \mathcal{N}(\mu,\,\sigma^{2})$를 style code $w \in W \subsetneq \mathbb{R}^{512}$에 대응시킨다. 
- Image2StyleGAN은 StyleGAN generator에 하나의 vector를 입력하는 대신 $k$개의 서로 다른 style code를 입력하는 방식을 제안했다. 이 확장된 space를 $W^k \subsetneq \mathbb{R}^{k \times 512}$라고 부르기로 하자.
- Style code를 정의하는 space를 $\mathbb{R}^{512} - W$까지 넓히면 복원 능력이 더 좋아진다. 새로 확장된 space는 StyleGAN의 $k$개 AdaIN에 입력되는 code가 서로 같다면 $W_\*$, 다르다면 $W_*^k$ space라고 부른다.

## The GAN Inversion Tradeoffs
다음으로 face editing 성능에 관련된 용어들을 정리하고 넘어가자. 
- 복원 대상 이미지와 복원 결과 이미지가 다른가? → Distortion
- 복원 결과 이미지가 사실적인가? → Perceptual quality
- 이미지에 대해서 유의미하고 사실적인 편집을 할 수 있는가? → Editability 

512-dimensional $W$ space에서 이미지를 표현하는데는 한계가 있기 때문에, 최근에는 이를 확장시킨 $W+$ space를 사용하는 추세이다. 이 방법으로 distortion을 줄일 수 있었지만, **distortion과 editability, perceptual quality 사이의 tradeoff**는 아직 해결이 필요하다. 아래  figure에서 두 가지 tradeoff를 찾아볼 수 있다.

**The distortion-perception and distortion-editability tradeoffs**
![tradeoffs](/assets/posts/face-editing/2022-05-03-review-E4E/tradeoffs.png) 
- 맨 왼쪽 열이 source, 두 번째 열과 마지막 열은 각각 pSp와 e4e를 사용해서 구한 style code set을 가지고 생성한 이미지이다. 그 사이 열에 있는 이미지들은 두 style code set를 interpolation해서 사용했다. 
- 즉, 왼쪽($W_*^k$)에서 오른쪽($\approx W$)으로 갈수록 distortion이 줄어든다.
- 첫 번째 열에서 오른쪽으로 갈수록 사람 얼굴이 더 자연스럽게 느껴진다. → distortion-perception tradeoff 
- 두 번째 열에서 오른쪽으로 갈수록 더 여성에 가까운 모습을 하고 있다. → distortion-editability tradeoff

# Motivation
**주어진 이미지에 대한 latent inversion이 $W$ space에 가까울수록 editability와 perceptual quality가 높아진다.** 여기서 '가깝다'란 서로 다른 18가지 style vector 사이의 variance가 작고, 각각의 vector가 $W$ 분포에 포함되어있음을 의미한다. 

# Encoder Design
Encoder4Editing은 아래 그림과 같은 구조를 하고 있다. 
![architecture](/assets/posts/face-editing/2022-05-03-review-E4E/architecture.png)

## Encoder-based Methology
지금까지 제시된 GAN Inversion 방법들은 크게 두 종류: **(i) latent code optimization**과 **(ii) encoder-based** 방법으로 구분할 수 있다. 본 논문에서는 (ii)를 선택했는데, single pass이라 inversion 속도가 빠르고 latent space 위 임의의 점($p \in \mathbb{R}^{k \times 512}$)에 mapping 될 확률이 낮기 때문이다.  

## Minimize Variation
**첫 번째 전략은 $W_{\*}^{k}$ space의 inversion이 $W_{*}$에 가까운 위치에 있도록 유도하는 것이다.** 다시 말해, 서로 다른 N(=18)개의 style code가 비슷해지도록 만들었다. 새로운 encoder는 N(=18)가지 style code 각각을 추정하기보다는, **첫번째 style code $w$와 그로부터의 offset N-1(=17)가지를 추정하도록 설계되었다.** 

$$ E(x) = (w, w+{\Delta}_1, \cdots, w+{\Delta}_{N-1}) $$ 

학습의 첫 단계에서 모든 offset을 0으로 초기화한 덕분에 $W_{\ast}$ space에서 시작해서 $W_{\ast}^{k}$ space로 확장해나가는 효과를 얻었다. 주어진 이미지에 대한 coarse reconstruction을 먼저 배우고, 후에 fine detail을 추가해나간다고 생각하면 쉽다. 또한, 아래와 같은 **L2 delta-regularization loss**를 사용해서 encoder output이 $W_{\ast}$ space 근처에 머물도록 강제시했다. 

$$ L_{d-reg}(w) = \sum_{n=1}^{N-1} 2^{-n} = 1$$

## Minimize Deviation From $W^k$
**두 번째 전략은 $W_{\*}^{k}$ space의 inversion이 $W^k$에 가까운 위치에 있도록 유도하는 것이다.** 즉, 모든 style code를 (StyleGAN mapping function에 의해 만들어진) 실제 $W$ 분포 또는 그 주변에서 가져오려 했다. 이를 위해서 주어진 style code가 $W$에 포함되는지  판별하는 **latent discriminator** $D_W$를 도입했다. 이 discriminator는 아래와 같은 minimax objective로 encoder와 함께 학습되었다.

$$ L_{adv}^{D} = - \mathbb{E}_{w \sim W}[\log {D_W(w)}] - \mathbb{E}_{w \sim p_X}[\log (1-{D_W({E(x)}_i)})] \\ 
+ \frac{\gamma}{2} \mathbb{E}_{w \sim W}[\Vert \nabla_w {D_W(w)} \Vert _2^2 ]$$

$$ L_{adv}^{D} = - \mathbb{E}_{w \sim p_X}[\log D_W({E(x)}_i)] $$

# Train
다른 GAN Inversion 모델들처럼 e4e도 distortion을 낮춰주는 loss function을 사용해서 학습했다. 거기에다 style code가 $W$ space 근처에 있도록 유도하는 loss function을 함께 사용해서 perceptual quality와 editability도 놓치지 않았다. 

$$ L(x) = \lambda_{dist}L_{dist}(x) + \lambda_{edit}L_{edit}(x) $$

## Distortion
$L_{dist}$는 아래와 같이 정의된다. 

$$ L_{dist}(x) = \lambda_{l2}L_2(x) + \lambda_{lpips}L_{LPIPS}(x)  + \lambda_{sim}L_{sim}(x) $$

각 항에 대해 알아보자면,
- $L_{sim}$은 pixel2Style2pixel(pSp)에서 소개된 identity loss를 확장한 개념이다. 기존 함수가 사람 얼굴에 특화되어있었다면, $L_{sim}$은 이미지의 도메인에 관계없이 사용된다. $<\cdots>$은 복원 대상 및 결과 이미지를 pretrained ResNet $C$에 통과시켜서 얻은 feature embedding으로 계산한 cosine similarity를 의미한다. 

$$ L_{sim}(x) = 1 - <C(x), C(G(e4e(x)))> $$

- $L_{LPIPS}$는 사람이 두 이미지를 얼마나 유사하다고 인지하는가를 반영하는 지표이다. LPIPS는 유사성을 판단하는 과정을 모방해서 설계되었다. 
- $L_{2}$는 복원 대상 및 결과 이미지의 pixel-wise difference를 말한다. 

## Perceptual Quality and Editability
$L_{edit}$는 아래와 같이 정의된다. 앞에서 이미 설명했기 때문에 각 항에 대한 설명은 생략하겠다.

$$ L_{edit}(x) = \lambda_{d-reg}L_{d-reg}(x) + \lambda_{adv}L_{adv}(x) $$

# Experiments
동일한 데이터셋으로 학습시킨 StyleGAN2 generator를 사용해서 이미지를 생성했다. Figure 위주로 소개해보려고 한다. 

**Latent code optimization vs. Encoder-based**
![opt-vs-enc](/assets/posts/face-editing/2022-05-03-review-E4E/opt-vs-enc.png) 
- Latent code optimization로 얻은 $w \in W_*^k$는 거의 완벽하게 이미지를 복원한다. 그러나 이미지 편집에 대해서 e4e를 사용했을 때만큼 드라마틱한 성능을 내지는 못한다.  

**Ablation study: Effects of $L_{edit}$**
![compare-configs](/assets/posts/face-editing/2022-05-03-review-E4E/compare-configs.png) 
- Configuration A는 $L_{dist}$ 만 사용했고, D는 $L_{editt}$ 까지 함께 사용해서 encoder를 학습시켰다. A encoder는 주어진 이미지를 $W_*^k$ space, D encoder는 $\approx W$ space의 style code set으로 mapping한다. 
- A encoder의 복원 성능이 더 좋다. 오른쪽 위의 말 예시에서, D encoder는 안장을 복원해내지 못했다.  
- D encoder의 편집 성능이 더 좋다. A style code set을 조작했을 때는 원치 않는 warping이 발생해서 사실적이지 못한 결과 이미지가 만들어진다. 왼쪽 위의 파란 자동차 예시와 오른쪽 아래의 고양이 예시에서 특히 두드러진다. 

**Latent Editing Consistency**
![consistency](/assets/posts/face-editing/2022-05-03-review-E4E/consistency.png)
- 원본 이미지에서 시작해서 GAN Inversion → Latent Manipulation을 2회 반복했을 때, 원본 이미지와 얼마나 비슷한 이미지가 생성되는지 확인했다. CycleGAN의 cycle-consistency 개념을 떠올리면 쉽다.
- A encoder를 사용한 경우에는 갑자기 결과 이미지에 안경이 생겨났다! D encoder로 생성한 결과 이미지가 원본 이미지와 완전히 같다고는 할 수 없지만, 분명히 A encoder보다 더 원본 이미지에 가깝다.  
